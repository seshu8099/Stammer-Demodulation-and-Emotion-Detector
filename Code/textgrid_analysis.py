# -*- coding: utf-8 -*-
"""TextGrid_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_uieEIvPWffpwuRYJ0XaMcKQuBF_9k7I
"""

!pip install praat-textgrids

!pip install textgrid

import textgrid

!pip install tgt

def fix_textgrid_header(text):
    if 'File type' not in text:
        header = 'File type = "ooTextFile"\nObject class = "TextGrid"\n\n'
        return header + text
    return text

def convert_and_clean_files(input_dir, output_dir):
    for filename in os.listdir(input_dir):
        if filename.endswith(".grid"):
            input_path = os.path.join(input_dir, filename)

            with open(input_path, "r", encoding="utf-8") as f:
                content = f.read()

            # Fix header if missing
            fixed_content = fix_textgrid_header(content)

            # Save as .TextGrid in output folder
            new_filename = filename.replace(".grid", ".TextGrid")
            output_path = os.path.join(output_dir, new_filename)

            with open(output_path, "w", encoding="utf-8") as f:
                f.write(fixed_content)

            print(f"✅ Converted and cleaned: {new_filename}")

convert_and_clean_files(original_folder, output_folder)

!pip install tgt

import tgt

def is_meaningful_textgrid(path):
    try:
        tg = tgt.read_textgrid(path, include_empty_intervals=True)
        for tier in tg.get_tiers():
            if len(tier.annotations) > 1:
                return True
            for ann in tier.annotations:
                if ann.text.strip():
                    return True
        return False
    except Exception as e:
        print(f"❌ Failed to read: {path}, error: {e}")
        return False

!cat "PATH TO YOUR FOLDER"

import os
import re
import pandas as pd
import matplotlib.pyplot as plt

# Step 1: Parse a single TextGrid file
def parse_textgrid(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    pattern = re.compile(r'^\s*([\d.]+)\s+"(.*)"')
    data = []

    for i in range(len(lines) - 1):
        match_start = pattern.match(lines[i])
        match_end = pattern.match(lines[i + 1])
        if match_start and match_end:
            start_time = float(match_start.group(1))
            label = match_start.group(2)
            end_time = float(match_end.group(1))
            data.append((start_time, end_time, label))

    return pd.DataFrame(data, columns=['start_time', 'end_time', 'label'])

# Step 2: Label classifier
def classify_label(label):
    if re.match(r'^\(.*\)$', label):
        return 'pause'
    elif re.match(r'^Q+\.?$', label) or 'QQ' in label or label == 'Q':
        return 'stammer'
    elif re.match(r'^:.*$', label):
        return 'word'
    elif re.match(r'^".*"$', label):
        return 'word'
    else:
        return 'word'

# Step 3: Plotting
def visualize_annotations(df, title):
    fig, ax = plt.subplots(figsize=(16, 6))
    color_map = {
        'word': 'skyblue',
        'pause': 'orange',
        'stammer': 'red',
    }

    for idx, row in df.iterrows():
        label_type = row['type']
        ax.plot([row['start_time'], row['end_time']], [idx, idx],
                color=color_map.get(label_type, 'gray'), linewidth=6)
        ax.text((row['start_time'] + row['end_time']) / 2, idx + 0.1,
                row['label'], fontsize=8, ha='center')

    ax.set_xlabel("Time (s)")
    ax.set_yticks([])
    ax.set_title(f"Annotation Timeline: {title}")
    plt.tight_layout()
    plt.show()

# Step 4: Loop over directory
def process_textgrids_in_directory(directory_path):
    for filename in os.listdir(directory_path):
        if filename.endswith(".TextGrid"):
            file_path = os.path.join(directory_path, filename)
            print(f"Processing: {filename}")

            df = parse_textgrid(file_path)
            df['type'] = df['label'].apply(classify_label)

            # Save parsed CSV
            output_csv = os.path.join(directory_path, filename.replace('.TextGrid', '_parsed.csv'))
            df.to_csv(output_csv, index=False)

            # Visualize
            visualize_annotations(df, filename)

# ---- Run It Here ----
directory_path = '/content/drive/MyDrive/Statter_Dataset/cleaned_grid'  # 🔁 Replace this with your actual path
process_textgrids_in_directory(directory_path)

import os
import re
import pandas as pd

# Step 1: Parse a single TextGrid file
def parse_textgrid(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    pattern = re.compile(r'^\s*([\d.]+)\s+"(.*)"')
    data = []

    for i in range(len(lines) - 1):
        match_start = pattern.match(lines[i])
        match_end = pattern.match(lines[i + 1])
        if match_start and match_end:
            start_time = float(match_start.group(1))
            label = match_start.group(2)
            end_time = float(match_end.group(1))
            data.append((start_time, end_time, label))

    return pd.DataFrame(data, columns=['start_time', 'end_time', 'label'])

# Step 2: Label classifier
def classify_label(label):
    if re.match(r'^\(.*\)$', label):
        return 'pause'
    elif re.match(r'^Q+\.?$', label) or 'QQ' in label or label == 'Q':
        return 'stammer'
    elif re.match(r'^:.*$', label):
        return 'word'
    elif re.match(r'^".*"$', label):
        return 'word'
    else:
        return 'word'

# Step 3: Process all files and combine
def combine_textgrid_data(directory_path, output_csv='combined_annotations.csv'):
    combined_df = pd.DataFrame()

    for filename in os.listdir(directory_path):
        if filename.endswith(".TextGrid"):
            file_path = os.path.join(directory_path, filename)
            print(f"Processing: {filename}")

            df = parse_textgrid(file_path)
            df['type'] = df['label'].apply(classify_label)
            df['file'] = filename  # Keep track of which file the entry came from

            combined_df = pd.concat([combined_df, df], ignore_index=True)

    combined_path = os.path.join(directory_path, output_csv)
    combined_df.to_csv(combined_path, index=False)
    print(f"\n✅ Combined CSV saved to: {combined_path}")

# ---- Run it here ----
directory_path = "PATH TO YOUR FOLDER"  #  Replace this with your folder path
combine_textgrid_data(directory_path)

# Count of each type
print(df['type'].value_counts())

# Plot: Distribution of annotation types
sns.countplot(data=df, x='type', order=df['type'].value_counts().index)
plt.title("Distribution of Annotation Types")
plt.show()

# Add a duration column
df['duration'] = df['end_time'] - df['start_time']

# Avg duration by type
print(df.groupby('type')['duration'].mean())

# Plot: Avg duration by type
sns.barplot(data=df, x='type', y='duration', estimator='mean')
plt.title("Average Duration by Type")
plt.show()

import pandas as pd

# Load your combined CSV
df = pd.read_csv(""PATH TO YOUR FOLDER"")

# Ensure proper types
df['start_time'] = pd.to_numeric(df['start_time'], errors='coerce')
df['end_time'] = pd.to_numeric(df['end_time'], errors='coerce')

# Duration of each interval
df['duration'] = df['end_time'] - df['start_time']

# Feature extraction function
def extract_features(group):
    total_duration = group['end_time'].max() - group['start_time'].min()
    num_words = (group['type'] == 'word').sum()
    num_stammers = (group['type'] == 'stammer').sum()
    num_pauses = (group['type'] == 'pause').sum()

    word_duration = group.loc[group['type'] == 'word', 'duration'].sum()
    stammer_duration = group.loc[group['type'] == 'stammer', 'duration'].sum()
    pause_duration = group.loc[group['type'] == 'pause', 'duration'].sum()

    return pd.Series({
        'total_duration': total_duration,
        'num_words': num_words,
        'num_stammers': num_stammers,
        'num_pauses': num_pauses,
        'word_rate': num_words / total_duration if total_duration > 0 else 0,
        'stammer_rate': num_stammers / total_duration if total_duration > 0 else 0,
        'pause_ratio': pause_duration / total_duration if total_duration > 0 else 0,
        'avg_word_duration': word_duration / num_words if num_words > 0 else 0,
        'avg_stammer_duration': stammer_duration / num_stammers if num_stammers > 0 else 0,
        'avg_pause_duration': pause_duration / num_pauses if num_pauses > 0 else 0,
    })

# Group by file and apply
features_df = df.groupby('file').apply(extract_features).reset_index()

# Save it
features_df.to_csv("speech_features.csv", index=False)
print("✅ Feature extraction complete. Saved as 'speech_features.csv'")

start_time = group['start_time'].min()
end_time = group['end_time'].max()

print(df.columns.tolist())

import pandas as pd
import numpy as np

# Load your combined annotations CSV
df = pd.read_csv("PATH TO YOUR FOLDER")

# Group by 'file' (which is the filename column in your case)
grouped = df.groupby('file')

# List to hold features per file
features = []

# Loop through each file's annotations
for filename, group in grouped:
    # Extract word and pause intervals
    total_words = len(group[group['type'] == 'word'])
    stammer_count = len(group[group['type'] == 'stammer'])
    pause_count = len(group[group['type'] == 'pause'])

    # Calculate stammer and pause ratios
    stammer_ratio = stammer_count / total_words if total_words > 0 else 0
    pause_ratio = pause_count / total_words if total_words > 0 else 0

    # Calculate total duration of the file
    start_time = group['start_time'].min()
    end_time = group['end_time'].max()
    duration = end_time - start_time

    # Calculate speech rate (words per second)
    speech_rate = total_words / duration if duration > 0 else 0

    # Calculate word durations and pause durations
    word_durations = []
    pause_durations = []

    for i in range(len(group) - 1):
        if group.iloc[i]['type'] == 'word' and group.iloc[i+1]['type'] == 'word':
            word_start = group.iloc[i]['start_time']
            word_end = group.iloc[i+1]['end_time']
            word_durations.append(word_end - word_start)

        if group.iloc[i]['type'] == 'pause' and group.iloc[i+1]['type'] == 'pause':
            pause_start = group.iloc[i]['start_time']
            pause_end = group.iloc[i+1]['end_time']
            pause_durations.append(pause_end - pause_start)

    avg_word_duration = np.mean(word_durations) if word_durations else 0
    std_word_duration = np.std(word_durations) if word_durations else 0
    avg_pause_duration = np.mean(pause_durations) if pause_durations else 0
    std_pause_duration = np.std(pause_durations) if pause_durations else 0
    total_pause_time = sum(pause_durations)

    # Add stammer level based on stammer ratio (0 = low, 1 = medium, 2 = high)
    stammer_level = pd.cut([stammer_ratio], bins=[-1, 0.1, 0.3, 1.0], labels=[0, 1, 2]).astype(int)[0]

    # Add readable label for stammer level
    stammer_label = {0: 'low', 1: 'medium', 2: 'high'}[stammer_level]

    # Append extracted features for the current file
    features.append({
        'filename': filename,
        'total_words': total_words,
        'stammer_count': stammer_count,
        'pause_count': pause_count,
        'stammer_ratio': stammer_ratio,
        'pause_ratio': pause_ratio,
        'duration_sec': duration,
        'speech_rate': speech_rate,
        'avg_word_duration': avg_word_duration,
        'std_word_duration': std_word_duration,
        'avg_pause_duration': avg_pause_duration,
        'std_pause_duration': std_pause_duration,
        'total_pause_time': total_pause_time,
        'stammer_level': stammer_level,
        'stammer_label': stammer_label
    })

# Convert the list of features into a DataFrame
features_df = pd.DataFrame(features)

# Save the extracted features to a CSV file
features_df.to_csv("PATH TO YOUR FOLDER", index=False)

print("✅ Features extracted and saved to 'features.csv'")

df

pip install praat-parselmouth

import pandas as pd

# Load the existing features file
features_path = "PATH TO YOUR FOLDER"
df = pd.read_csv(features_path)

# Add stammer level (0 = low, 1 = medium, 2 = high)
df['stammer_level'] = pd.cut(df['stammer_ratio'],
                             bins=[-1, 0.1, 0.3, 1.0],
                             labels=[0, 1, 2]).astype(int)

# Add readable label for stammer level
df['stammer_label'] = df['stammer_level'].map({0: 'low', 1: 'medium', 2: 'high'})

# Save it back to the same file (inplace update)
df.to_csv(features_path, index=False)

print("✅ Updated 'features.csv' with stammer levels and labels.")

print(type(df))
print(df.head())

null_val = df.isnull().count()
print(null_val)

"""# Handling missing values"""

df = df.dropna()  # or use df.fillna() with appropriate values

import seaborn as sns
import matplotlib.pyplot as plt

# Load the features
df = pd.read_csv"PATH TO YOUR FOLDER"

# Example: Plot stammer vs pause ratio
sns.scatterplot(data=df, x='stammer_ratio', y='pause_ratio')
plt.title("Stammer Ratio vs Pause Ratio")
plt.show()

"""# 📊 1. Histogram: Distribution of Stammer Ratio


"""

plt.figure(figsize=(8, 5))
sns.histplot(df['stammer_ratio'], bins=30, kde=True, color='skyblue')
plt.title('Distribution of Stammer Ratio')
plt.xlabel('Stammer Ratio')
plt.ylabel('Frequency')
plt.grid(True)
plt.show()

"""# 📊 2. Histogram: Speech Rate Distribution"""

plt.figure(figsize=(8, 5))
sns.histplot(df['speech_rate'], bins=30, kde=True, color='salmon')
plt.title('Speech Rate Distribution')
plt.xlabel('Words per Second')
plt.ylabel('Count')
plt.grid(True)
plt.show()

"""
# 🧮 3. Scatter Plot: Total Words vs Stammer Count **bold text**"""

plt.figure(figsize=(8, 5))
sns.scatterplot(data=df, x='total_words', y='stammer_count', color='purple')
plt.title('Total Words vs Stammer Count')
plt.xlabel('Total Words')
plt.ylabel('Stammer Count')
plt.grid(True)
plt.show()

"""
# 🔢 4. Scatter Plot: Duration vs Pause Count"""

plt.figure(figsize=(8, 5))
sns.scatterplot(data=df, x='duration_sec', y='pause_count', color='orange')
plt.title('Speech Duration vs Pause Count')
plt.xlabel('Duration (seconds)')
plt.ylabel('Pause Count')
plt.grid(True)
plt.show()

"""
# 📈 5. Line Plot: Speech Rate Across Files (sorted)"""

df_sorted = df.sort_values(by='speech_rate').reset_index()
plt.figure(figsize=(10, 5))
sns.lineplot(data=df_sorted, x=df_sorted.index, y='speech_rate', marker='o', color='teal')
plt.title('Speech Rate Across Files')
plt.xlabel('File Index (sorted)')
plt.ylabel('Speech Rate')
plt.grid(True)
plt.show()

"""
# 🔳 6. Correlation Heatmap"""

plt.figure(figsize=(8, 6))
sns.heatmap(df[['total_words', 'stammer_count', 'pause_count', 'stammer_ratio', 'pause_ratio', 'duration_sec', 'speech_rate']].corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Feature Correlation Heatmap')
plt.show()

"""
# 🎯 7. Box Plot: Stammer Ratio Variability"""

plt.figure(figsize=(8, 5))
sns.boxplot(data=df, y='stammer_ratio', color='lightgreen')
plt.title('Boxplot of Stammer Ratio')
plt.ylabel('Stammer Ratio')
plt.grid(True)
plt.show()

"""
# 🔍 8. Highlight High Stammer Ratio Files"""

high_stammer = df[df['stammer_ratio'] > 0.2]  # Customize threshold
display(high_stammer[['filename', 'stammer_ratio', 'total_words']])

"""
# 🎨 9. Joint Plot: Speech Rate vs Stammer Ratio"""

sns.jointplot(data=df, x='speech_rate', y='stammer_ratio', kind='hex', color='coral', height=6)
plt.suptitle("Speech Rate vs Stammer Ratio", y=1.02)
plt.show()

"""
# ⏱️ 10. Pie Chart: Average Time Distribution"""

avg_dur = df[['duration_sec', 'pause_count']].mean()
avg_speech_time = avg_dur['duration_sec'] - (avg_dur['pause_count'] * 0.5)  # assuming avg pause = 0.5 sec

plt.figure(figsize=(6, 6))
plt.pie([avg_speech_time, avg_dur['pause_count'] * 0.5], labels=['Speech', 'Pause'], autopct='%1.1f%%', colors=['#66b3ff','#ff9999'])
plt.title("Average Time Spent Speaking vs Pausing")
plt.show()

'''from scipy.stats import zscore
df = df[(np.abs(zscore(df.select_dtypes(include=[np.number]))) < 3).all(axis=1)]
'''

df.head()



"""
# ✅ Preprocessing:
Normalization & Label Encoding"""

import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from imblearn.over_sampling import SMOTE
from collections import Counter

# Load the features CSV
df = pd.read_csv("PATH TO YOUR FOLDER")

# Label Encoding for 'stammer_label' (binary: non-stammered = 0, stammered = 1)
df['stammer_label_encoded'] = df['stammer_label'].apply(lambda x: 1 if x == 'stammer' else 0)

# Label Encoding for 'stammer_level' (low, medium, high)
le = LabelEncoder()
df['stammer_level_encoded'] = le.fit_transform(df['stammer_level'])

# Drop the original labels
df = df.drop(columns=['stammer_label', 'stammer_level'])

# Define features (X)
X = df[['total_words', 'stammer_count', 'pause_count', 'stammer_ratio', 'pause_ratio',
        'duration_sec', 'speech_rate', 'avg_word_duration', 'std_word_duration',
        'avg_pause_duration', 'std_pause_duration', 'total_pause_time']]

# Define targets (y) for multi-output classification
y_stammer = df['stammer_label_encoded']
y_level = df['stammer_level_encoded']

# Split data into training and testing sets (80% training, 20% testing)
X_train, X_test, y_train_stammer, y_test_stammer, y_train_level, y_test_level = train_test_split(
    X, y_stammer, y_level, test_size=0.2, random_state=42)

# Check if there's more than one class in the training target for stammer detection
print("Stammer Detection Class Distribution (Before SMOTE):")
print(Counter(y_train_stammer))

# If only one class is present, either handle this issue or avoid resampling
if len(Counter(y_train_stammer)) > 1:
    # Normalize the features
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # Apply SMOTE for the stammer detection task (for balancing)
    smote_stammer = SMOTE(random_state=42, k_neighbors=2)
    X_train_resampled_stammer, y_train_resampled_stammer = smote_stammer.fit_resample(X_train_scaled, y_train_stammer)

    # Apply SMOTE for the severity level classification task (for balancing)
    smote_level = SMOTE(random_state=42, k_neighbors=2)
    X_train_resampled_level, y_train_resampled_level = smote_level.fit_resample(X_train_scaled, y_train_level)

    # Check the class distribution after SMOTE
    print("Stammer Detection Class Distribution (After SMOTE):")
    print(Counter(y_train_resampled_stammer))

    print("Severity Level Classification Class Distribution (After SMOTE):")
    print(Counter(y_train_resampled_level))

    # Initialize Random Forest Classifier for both tasks
    rf_stammer = RandomForestClassifier(n_estimators=100, random_state=42)
    rf_level = RandomForestClassifier(n_estimators=100, random_state=42)

    # Train the models with the resampled data
    rf_stammer.fit(X_train_resampled_stammer, y_train_resampled_stammer)
    rf_level.fit(X_train_resampled_level, y_train_resampled_level)

    # Make predictions for both tasks
    y_pred_stammer = rf_stammer.predict(X_test_scaled)
    y_pred_level = rf_level.predict(X_test_scaled)

    # Evaluate the models
    print("Stammer Detection Classification Report:")
    print(classification_report(y_test_stammer, y_pred_stammer))

    print("Stammer Severity Level Classification Report:")
    print(classification_report(y_test_level, y_pred_level))

else:
    print("Insufficient class distribution for stammer detection. Skipping SMOTE.")

import matplotlib.pyplot as plt

# Check the class distribution for both tasks
print("Stammer Detection Class Distribution (Before SMOTE):")
print(y_train_stammer.value_counts())

print("Stammer Severity Level Class Distribution (Before SMOTE):")
print(y_train_level.value_counts())

# Visualize the class distributions
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
y_train_stammer.value_counts().plot(kind='bar', title='Stammer Detection Class Distribution', color=['#ff9999', '#66b3ff'])
plt.ylabel('Frequency')
plt.xlabel('Class')

plt.subplot(1, 2, 2)
y_train_level.value_counts().plot(kind='bar', title='Stammer Severity Level Class Distribution', color=['#ff9999', '#66b3ff'])
plt.ylabel('Frequency')
plt.xlabel('Class')

plt

from sklearn.ensemble import RandomForestClassifier

# Initialize Random Forest Classifier with class weights
rf_stammer = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
rf_level = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')

# Train the models without SMOTE (still considering class weights)
rf_stammer.fit(X_train_scaled, y_train_stammer)
rf_level.fit(X_train_scaled, y_train_level)

# Make predictions for both tasks
y_pred_stammer = rf_stammer.predict(X_test_scaled)
y_pred_level = rf_level.predict(X_test_scaled)

# Evaluate the models
print("Stammer Detection Classification Report:")
print(classification_report(y_test_stammer, y_pred_stammer))

print("Stammer Severity Level Classification Report:")
print(classification_report(y_test_level, y_pred_level))